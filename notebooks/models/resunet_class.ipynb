{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..//..\")\n",
    "import config\n",
    "from utils import compute_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set device to GPU\n",
    "dev = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load frames idx detail\n",
    "frames_idx = pd.read_csv(config.TR_FRAMES_IDX, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load frames deforestation area history\n",
    "deforestation = pd.read_csv(config.TR_DEFORESTATION, index_col=0)\n",
    "deforestation[\"date\"] = pd.to_datetime(deforestation[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create limits history grid\n",
    "time_grid = np.zeros((len(config.TIME_STEPS), frames_idx[\"x\"].max() - frames_idx[\"x\"].min() + 1, frames_idx[\"y\"].max() - frames_idx[\"y\"].min() + 1))\n",
    "for t, dt in tqdm(enumerate(config.TIME_STEPS), total=len(config.TIME_STEPS)):\n",
    "    defor_area = (\n",
    "        deforestation[\n",
    "            deforestation[\"date\"] == dt\n",
    "        ].set_index(\"frame_id\")[\"area\"] +\\\n",
    "        pd.Series(0, index=frames_idx.index)\n",
    "    ).fillna(0).sort_index()\n",
    "    time_grid[t, :, :] = defor_area.values.reshape(time_grid[0, :, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.lineplot((time_grid>0).mean(axis=(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_grid[time_grid>0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(time_grid == 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.histplot(time_grid[time_grid>0].flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Past Deforestation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "past_defor = pd.read_csv(config.TR_PAST_DEFOR, index_col=0)\n",
    "print(\"Shape:\", past_defor.shape)\n",
    "past_defor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "past_defor[\"date\"] = pd.to_datetime(past_defor[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "past_grid = np.zeros((len(config.TIME_STEPS), frames_idx[\"x\"].max() - frames_idx[\"x\"].min() + 1, frames_idx[\"y\"].max() - frames_idx[\"y\"].min() + 1))\n",
    "for t, dt in tqdm(enumerate(config.TIME_STEPS), total=len(config.TIME_STEPS)):\n",
    "    past_area = (\n",
    "        past_defor[\n",
    "            past_defor[\"date\"] <= dt\n",
    "        ].groupby(\"frame_id\")[\"area\"].sum() +\\\n",
    "        pd.Series(0, index=frames_idx.index)\n",
    "    ).fillna(0).sort_index()\n",
    "    past_grid[t, :, :] = past_area.values.reshape(time_grid[0, :, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clip on 1.0 (upper bound)\n",
    "past_grid = past_grid.clip(max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "past_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.lineplot((past_grid>0).mean(axis=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBAMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ibama = pd.read_csv(config.TR_IBAMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ibama.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = ['Access_Minut_Beef_2012', 'Access_Minut_City',\n",
    "       'Access_Minut_soy', 'Access_Minut_soy_p25', 'Access_Minut_wood_2012',\n",
    "       'garimpos', 'Multas_upto2019', 'Terras_Devolutas', 'TI_Dist',\n",
    "       'UCPI_dist', 'UCUS_Dist', 'Pasture_Mapbiomas', 'Soybean_Mapbiomas',\n",
    "       'UCPI_IO', 'UCUS_IO', 'Terras_Devolutas_IO', 'TI_IO']\n",
    "ib_array = np.zeros((len(cols), frames_idx[\"x\"].max() - frames_idx[\"x\"].min() + 1, frames_idx[\"y\"].max() - frames_idx[\"y\"].min() + 1))\n",
    "for icol, col in tqdm(enumerate(cols), total=len(cols)):\n",
    "    v = (\n",
    "        ibama.set_index(\"frame_id\")[col] +\\\n",
    "        pd.Series(0, index=frames_idx.index)\n",
    "    ).sort_index()\n",
    "    v = v.fillna(v.mean())\n",
    "    ib_array[icol, :, :] = v.values.reshape(ib_array[0, :, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ib_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ib_array.mean(axis=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counties data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# counties\n",
    "frames_county = pd.read_csv(config.TR_COUNTIES, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "county_data = np.zeros((2, frames_idx[\"x\"].max() - frames_idx[\"x\"].min() + 1, frames_idx[\"y\"].max() - frames_idx[\"y\"].min() + 1))\n",
    "county_data[0, :, :] = (\n",
    "    frames_county.set_index(\"frame_id\")[\"populacao\"] +\\\n",
    "    pd.Series(0, index=frames_idx.index)\n",
    ").fillna(0).\\\n",
    "    values.reshape(county_data.shape[1:])\n",
    "\n",
    "county_data[1, :, :] = (\n",
    "    frames_county.set_index(\"frame_id\")[\"densidade\"] +\\\n",
    "    pd.Series(0, index=frames_idx.index)\n",
    ").fillna(0).\\\n",
    "    values.reshape(county_data.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "county_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "county_data.mean(axis=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute frame patches\n",
    "\n",
    "A patch is squared set of unitary frames. The patch formation process consists in iterating through the full image on both axis computing the frames corresponding to each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config.INPUT_BOXES_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_condition = \"both\"  # deforestation | borders | both\n",
    "\n",
    "bundle_step = 64\n",
    "patches = []\n",
    "patches_full = []\n",
    "for ix in tqdm(list(range(frames_idx[\"x\"].min(), frames_idx[\"x\"].max()+1, bundle_step))):\n",
    "    fx = ix + config.INPUT_BOXES_SIZE\n",
    "    for iy in range(frames_idx[\"y\"].min(), frames_idx[\"y\"].max()+1, bundle_step):\n",
    "        fy = iy + config.INPUT_BOXES_SIZE\n",
    "\n",
    "        iframes = frames_idx[\n",
    "            (frames_idx[\"x\"] >= ix) & \n",
    "            (frames_idx[\"x\"] < fx) &\n",
    "            (frames_idx[\"y\"] >= iy) &\n",
    "            (frames_idx[\"y\"] < fy)\n",
    "        ]\n",
    "\n",
    "        patches_full.append(iframes.index)\n",
    "        \n",
    "        if out_condition == \"borders\":\n",
    "            if iframes[\"in_borders\"].mean() >= 0.5:  # condition: bundle has to be at least half inside borders\n",
    "                patches.append(iframes.index)\n",
    "                \n",
    "        elif out_condition == \"deforestation\":\n",
    "            out_of_borders_frames = len(set(iframes.index) - set(deforestation[\"frame_id\"].values))\n",
    "            if out_of_borders_frames < len(iframes):  # condition: bundle has to contain some deforestation\n",
    "                patches.append(iframes.index) \n",
    "\n",
    "        elif out_condition == \"both\":\n",
    "            out_of_borders_frames = len(set(iframes.index) - set(deforestation[\"frame_id\"].values))\n",
    "            if (out_of_borders_frames < len(iframes)) and (iframes[\"in_borders\"].mean() >= 0.5):\n",
    "                patches.append(iframes.index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(patches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove patches that represent reduced regions\n",
    "patches = [b for b in patches if (len(b)==config.INPUT_BOXES_SIZE**2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove not used anymore dataframes\n",
    "del deforestation\n",
    "del past_defor\n",
    "del ibama\n",
    "del frames_county"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_time_idx = range(0,156)\n",
    "val_time_idx = range(104, 206)\n",
    "test_time_idx = range(154, 258)\n",
    "\n",
    "train_data = time_grid[train_time_idx, :, :]\n",
    "val_data = time_grid[val_time_idx, :, :]\n",
    "test_data = time_grid[test_time_idx, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "train : {config.TIME_STEPS[train_time_idx[52]].date()} -> {config.TIME_STEPS[train_time_idx[-1]].date()}\n",
    "val   : {config.TIME_STEPS[val_time_idx[52]].date()} -> {config.TIME_STEPS[val_time_idx[-1]].date()}\n",
    "test  : {config.TIME_STEPS[test_time_idx[52]].date()} -> {config.TIME_STEPS[test_time_idx[-1]].date()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config.TIME_STEPS[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n1 = (train_data <= 1e-18).sum() - ((~frames_idx[\"in_borders\"]).sum() * len(train_time_idx))\n",
    "n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(train_data[(train_data > 1e-12)], bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n2 = (train_data > 1e-12).sum()\n",
    "n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imbalance_factor = n1 / n2\n",
    "imbalance_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(ib_array.shape[0]):\n",
    "    ib_array[i, :, :] = (ib_array[i, :, :] - ib_array[i, :, :].mean()) / ib_array[i, :, :].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pop = (county_data[0, :, :] - np.median(county_data[0, :, :])) / 1e5\n",
    "norm_den = (county_data[1, :, :] - np.median(county_data[1, :, :])) / 30\n",
    "\n",
    "county_data[0, :, :] = norm_pop\n",
    "county_data[1, :, :] = norm_den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# focal loss\n",
    "from segmentation_models_pytorch.losses import FocalLoss\n",
    "loss = FocalLoss(\"binary\", gamma=3, reduction=\"mean\").to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FUTURE_WINDOW_PRED = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        X, \n",
    "        patches, \n",
    "        frames_idx,\n",
    "        county_data=None,\n",
    "        ibama_data=None,\n",
    "        past_defor_data=None\n",
    "    ):\n",
    "        super(CustomDataset, self).__init__()\n",
    "\n",
    "        self.patches = patches\n",
    "        self.frames_idx = frames_idx\n",
    "        self.X = X\n",
    "        self.county_data = county_data\n",
    "        self.ibama_data = ibama_data\n",
    "        self.past_defor_data = past_defor_data\n",
    "\n",
    "        self.autor_window = 52\n",
    "        self.future_window = FUTURE_WINDOW_PRED\n",
    "        self.ix = frames_idx[\"x\"].min()\n",
    "        self.iy = frames_idx[\"y\"].min()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches) * (self.X.shape[0]-self.autor_window-self.future_window+1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # get index info\n",
    "        idx_patch = index // (self.X.shape[0]-self.autor_window-self.future_window+1)\n",
    "        idx_time   = index % (self.X.shape[0]-self.autor_window-self.future_window+1)\n",
    "        idx_frames = self.frames_idx.loc[self.patches[idx_patch]]\n",
    "\n",
    "        # get input\n",
    "        \n",
    "        # full past deforestation\n",
    "        input_matrix = self.past_defor_data[\n",
    "            idx_time+self.autor_window, \n",
    "            idx_frames[\"x\"].min()-self.ix:idx_frames[\"x\"].max()-self.ix+1, \n",
    "            idx_frames[\"y\"].min()-self.iy:idx_frames[\"y\"].max()-self.iy+1\n",
    "        ]\n",
    "        input_matrix = input_matrix.reshape(1, input_matrix.shape[0], input_matrix.shape[1])\n",
    "        \n",
    "        # # last 'autor_window' weeks of deforestation\n",
    "        # input_matrix = self.X[\n",
    "        #     idx_time:idx_time+self.autor_window, \n",
    "        #     idx_frames[\"x\"].min()-self.ix:idx_frames[\"x\"].max()-self.ix+1, \n",
    "        #     idx_frames[\"y\"].min()-self.iy:idx_frames[\"y\"].max()-self.iy+1\n",
    "        # ].sum(axis=0).clip(max=1.0)\n",
    "        # input_matrix = input_matrix.reshape(1, input_matrix.shape[0], input_matrix.shape[1])\n",
    "        \n",
    "        # some 'delta weeks' of deforestation\n",
    "        lagweeks = [52, 24, 12, 4, 2, 0]\n",
    "        for i in range(len(lagweeks)-1):\n",
    "            input_matrix = np.concatenate([\n",
    "                input_matrix,\n",
    "                self.X[\n",
    "                    idx_time+self.autor_window-lagweeks[i]:idx_time+self.autor_window-lagweeks[i+1],\n",
    "                    idx_frames[\"x\"].min()-self.ix:idx_frames[\"x\"].max()-self.ix+1, \n",
    "                    idx_frames[\"y\"].min()-self.iy:idx_frames[\"y\"].max()-self.iy+1\n",
    "                ].sum(axis=0).clip(max=1.0)\\\n",
    "                .reshape((1, input_matrix.shape[1], input_matrix.shape[2]))\n",
    "            ])\n",
    "        \n",
    "        if self.county_data is not None:\n",
    "            input_matrix = np.concatenate([\n",
    "                input_matrix,\n",
    "                self.county_data[\n",
    "                    :,\n",
    "                    idx_frames[\"x\"].min()-self.ix:idx_frames[\"x\"].max()-self.ix+1, \n",
    "                    idx_frames[\"y\"].min()-self.iy:idx_frames[\"y\"].max()-self.iy+1\n",
    "                ]\n",
    "            ])\n",
    "        \n",
    "        \n",
    "        if self.ibama_data is not None:\n",
    "            input_matrix = np.concatenate([\n",
    "                input_matrix,\n",
    "                self.ibama_data[\n",
    "                    :,\n",
    "                    idx_frames[\"x\"].min()-self.ix:idx_frames[\"x\"].max()-self.ix+1, \n",
    "                    idx_frames[\"y\"].min()-self.iy:idx_frames[\"y\"].max()-self.iy+1\n",
    "                ]\n",
    "            ])\n",
    "            \n",
    "        data = torch.tensor(input_matrix).float().to(dev)\n",
    "\n",
    "        # get output\n",
    "        labels = np.zeros(\n",
    "            (\n",
    "                idx_frames[\"x\"].max()-idx_frames[\"x\"].min() + 1, \n",
    "                idx_frames[\"y\"].max()-idx_frames[\"y\"].min() + 1\n",
    "            )\n",
    "        )\n",
    "        target_idx = np.where(\n",
    "            self.X[\n",
    "                idx_time+self.autor_window:idx_time+self.autor_window+self.future_window, \n",
    "                idx_frames[\"x\"].min()-self.ix:idx_frames[\"x\"].max()-self.ix+1, \n",
    "                idx_frames[\"y\"].min()-self.iy:idx_frames[\"y\"].max()-self.iy+1\n",
    "            ].sum(axis=0) > 1e-18\n",
    "        )\n",
    "        labels[target_idx] = 1\n",
    "        labels = torch.tensor(labels).float().to(dev)\n",
    "        \n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patches_sample_train = patches\n",
    "patches_sample_val = patches\n",
    "\n",
    "# rand_patches_idx = np.random.choice(range(len(patches)), 60)\n",
    "# patches_sample_train = [patches[idx] for idx in rand_patches_idx]\n",
    "# patches_sample_val = [patches[idx] for idx in rand_patches_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "    CustomDataset(\n",
    "        train_data, \n",
    "        patches_sample_train, \n",
    "        frames_idx,\n",
    "        past_defor_data=past_grid,\n",
    "        county_data=county_data,\n",
    "        ibama_data=ib_array\n",
    "    ),\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    CustomDataset(\n",
    "        val_data, \n",
    "        patches_sample_val, \n",
    "        frames_idx,\n",
    "        past_defor_data=past_grid,\n",
    "        county_data=county_data,\n",
    "        ibama_data=ib_array\n",
    "    ),\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainloader.__len__(), valloader.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "Evaluate error without any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# baseline: all zero\n",
    "base_train_err = 0\n",
    "for inputs, labels in tqdm(trainloader):\n",
    "    y_pred = torch.tensor(-10*np.ones(labels.shape)).to(dev)\n",
    "    base_train_err += loss(y_pred=y_pred, y_true=labels)\n",
    "base_train_err = base_train_err / len(trainloader)\n",
    "\n",
    "print(f\"Baseline Error (Train) = {base_train_err:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_val_err = 0\n",
    "for inputs, labels in tqdm(valloader):\n",
    "    y_pred = torch.tensor(-10*np.ones(labels.shape)).to(dev)\n",
    "    base_val_err += loss(y_pred=y_pred, y_true=labels)\n",
    "base_val_err = base_val_err / len(valloader)\n",
    "print(f\"Baseline Error (Validation) = {base_val_err:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from resunet import ResUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_channels = 25\n",
    "in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ResUnet(\n",
    "    channel=in_channels,\n",
    "    output_dim=1,\n",
    "    filters=[16, 32, 64, 128]\n",
    ").to(dev)\n",
    "optimizer = optim.Adam(model.parameters())#, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([p.flatten().shape[0] for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train loop\n",
    "model.epoch = 0\n",
    "model.errs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    err = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        err += loss(model(inputs), labels).detach()\n",
    "    err = err / len(dataloader)\n",
    "\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_epoch():\n",
    "    model.epoch += 1\n",
    "    print(f\"\\nEpoch {model.epoch}\")\n",
    "    \n",
    "    train_err = 0\n",
    "    for inputs, labels in tqdm(trainloader):\n",
    "        L = loss(model(inputs), labels)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        train_err += L.detach()\n",
    "    train_err = train_err / len(trainloader)\n",
    "    \n",
    "    return train_err\n",
    "\n",
    "\n",
    "def train(n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # train for 1 epoch and compute error\n",
    "        train_err = run_epoch()\n",
    "\n",
    "        # compute validation error and save history            \n",
    "        val_err = evaluate_model(model, valloader)\n",
    "        model.errs.append([train_err, val_err])\n",
    "\n",
    "        print(f\"Epoch {model.epoch}: Train Loss = {train_err:.6f} | Validation Loss = {val_err:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    float((model.errs[-1][0] - base_train_err) / base_train_err), \n",
    "    float((model.errs[-1][1] - base_val_err) / base_val_err)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "x = np.array(range(len(model.errs))) + 1\n",
    "sns.lineplot(x=x, y=[float(e[0]) for e in model.errs], label=\"Train\")\n",
    "sns.lineplot(x=x, y=[float(e[1]) for e in model.errs], label=\"Validation\")\n",
    "ax.set_title(\"Loss\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(\n",
    "    CustomDataset(\n",
    "        test_data, \n",
    "        patches_sample_val, \n",
    "        frames_idx,\n",
    "        county_data=county_data,\n",
    "        ibama_data=ib_array,\n",
    "        past_defor_data=past_grid\n",
    "    ),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: erro por true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sample(min_area=0.1, dataloader=trainloader):\n",
    "    for input_, truth in dataloader:\n",
    "        batches = list(range(truth.shape[0])) \n",
    "        shuffle(batches)\n",
    "        for idx_batch in batches:\n",
    "            if truth[idx_batch, :, :].mean() >= min_area:\n",
    "                return input_, truth, idx_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sigmoid_fun = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_, truth, idx_batch = get_sample(1e-3, testloader)\n",
    "\n",
    "pred = sigmoid_fun(model(input_))[idx_batch, 0, :, :]\n",
    "label = truth[idx_batch, :, :].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 6))\n",
    "m1 = ax[0].matshow(label, cmap=\"Reds\", vmin=0, vmax=1)\n",
    "m2 = ax[1].matshow(pred.detach().cpu().numpy(), cmap=\"Reds\", vmin=0, vmax=1)\n",
    "m3 = ax[2].matshow(pred.detach().cpu().numpy() - label.cpu().numpy(), cmap=\"seismic\", vmin=-1, vmax=1)\n",
    "fig.colorbar(m1)\n",
    "fig.colorbar(m2)\n",
    "fig.colorbar(m3)\n",
    "ax[0].set_title(\"True\")\n",
    "ax[1].set_title(\"Prediction\")\n",
    "ax[2].set_title(\"Error\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize=(24, 8))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        ax[j, i].matshow(input_[idx_batch, i+5*j, :, :].cpu() > 0, cmap=\"Reds\", vmin=truth.min(), vmax=truth.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treshold Selection\n",
    "\n",
    "Use validation dataset to select the treshold that maximizes F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pt_values = range(0, 101, 1)\n",
    "val_p_cm = np.zeros((len(pt_values), 2, 2)).astype(int)\n",
    "for inputs, labels in tqdm(valloader):\n",
    "    # compute prediction (probability)\n",
    "    p_hat = sigmoid_fun(model(inputs))[:, 0, :, :].detach().cpu().numpy().flatten()\n",
    "    vals = labels[:, :, :].cpu().numpy().flatten()\n",
    "    for i, ptresh in enumerate(pt_values):\n",
    "        # for each treshold compute confusion matrix\n",
    "        y_hat = p_hat >= ptresh / 100\n",
    "        unique_0, counts_0 = np.unique(y_hat[np.where(vals == 0)], return_counts=True)\n",
    "        unique_1, counts_1 = np.unique(y_hat[np.where(vals == 1)], return_counts=True)\n",
    "        for (u, c) in zip(unique_0, counts_0):\n",
    "            if u:  # pred True\n",
    "                val_p_cm[i, 0, 1] += c\n",
    "            else:\n",
    "                val_p_cm[i, 0, 0] += c\n",
    "        for (u, c) in zip(unique_1, counts_1):\n",
    "            if u:\n",
    "                val_p_cm[i, 1, 1] += c\n",
    "            else:\n",
    "                val_p_cm[i, 1, 0] += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc = (val_p_cm[:, 0, 0] + val_p_cm[:, 1, 1]) / val_p_cm.sum(axis=(1,2))\n",
    "prc = val_p_cm[:, 1, 1] / val_p_cm[:, :, 1].sum(axis=1)\n",
    "prc[np.where(prc!=prc)] = 1e-8\n",
    "rcl = val_p_cm[:, 1, 1] / val_p_cm[:, 1, :].sum(axis=1)\n",
    "f1 = 2 * prc * rcl / (prc + rcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(18, 9), sharex=True)\n",
    "sns.lineplot(x=pt_values, y=acc, ax=ax[0, 0])\n",
    "sns.lineplot(x=pt_values, y=prc, ax=ax[0, 1])\n",
    "sns.lineplot(x=pt_values, y=rcl, ax=ax[1, 0])\n",
    "sns.lineplot(x=pt_values, y=f1, ax=ax[1, 1])\n",
    "ax[0,0].set_title(\"Accuracy\")\n",
    "ax[0,1].set_title(\"Precision\")\n",
    "ax[1,0].set_title(\"Recall\")\n",
    "ax[1,1].set_title(\"F1-Score\")\n",
    "sns.lineplot(x=[0,100], y=[0,1], ax=ax[0,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i_treshold = np.argmax(f1[f1>0])\n",
    "ptreshold = pt_values[i_treshold]\n",
    "ptreshold, f1[i_treshold], prc[i_treshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_p_cm[i_treshold, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Score\n",
    "\n",
    "Apply chosen treshold and compute scores in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_cm = np.zeros((2,2)).astype(int)\n",
    "for inputs, labels in tqdm(testloader):\n",
    "    pred = (sigmoid_fun(model(inputs)).detach() >= ptreshold / 100).cpu().numpy().flatten()\n",
    "    vals = labels.cpu().numpy().flatten()\n",
    "    unique_0, counts_0 = np.unique(pred[np.where(vals == 0)], return_counts=True)\n",
    "    unique_1, counts_1 = np.unique(pred[np.where(vals == 1)], return_counts=True)\n",
    "    for (u, c) in zip(unique_0, counts_0):\n",
    "        if u:  # pred True\n",
    "            test_cm[0, 1] += c\n",
    "        else:\n",
    "            test_cm[0, 0] += c\n",
    "    for (u, c) in zip(unique_1, counts_1):\n",
    "        if u:\n",
    "            test_cm[1, 1] += c\n",
    "        else:\n",
    "            test_cm[1, 0] += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_test = (test_cm[0, 0] + test_cm[1, 1]) / test_cm.sum()\n",
    "prc_test = test_cm[1, 1] / test_cm[:, 1].sum()\n",
    "rcl_test = test_cm[1, 1] / test_cm[1, :].sum()\n",
    "f1_test = 2 * prc_test * rcl_test / (prc_test + rcl_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_test, prc_test, rcl_test, f1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lower spatial granularity\n",
    "\n",
    "Apply a spatial reduction to data, select new treshold (in validation) and evaluate in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "def apply_distance_threshold(arr, threshold):\n",
    "    # Define a kernel that considers neighboring cells within the threshold distance\n",
    "    kernel_size = 2 * threshold + 1\n",
    "    kernel = np.ones((kernel_size, kernel_size))\n",
    "    \n",
    "    # Convolve the original array with the kernel\n",
    "    convolved = convolve(arr, kernel, mode='constant', cval=0)\n",
    "    \n",
    "    # Apply threshold to the convolved array\n",
    "    result = convolved > 0\n",
    "    \n",
    "    return result.astype(int)\n",
    "\n",
    "def apply_distance_threshold_3d(arr_3d, threshold):\n",
    "    result_3d = np.zeros_like(arr_3d)\n",
    "    \n",
    "    for i, arr_2d in enumerate(arr_3d):\n",
    "        result_3d[i] = apply_distance_threshold(arr_2d, threshold)\n",
    "    \n",
    "    return result_3d\n",
    "\n",
    "# Example usage\n",
    "original_array_3d = np.array([[[0, 0, 0, 0, 0],\n",
    "                                [0, 0, 0, 0, 0],\n",
    "                                [0, 0, 0, 0, 0]],\n",
    "                               \n",
    "                               [[0, 0, 0, 1, 0],\n",
    "                                [0, 0, 0, 1, 0],\n",
    "                                [0, 0, 0, 0, 0]]])\n",
    "\n",
    "threshold_distance = 1\n",
    "\n",
    "result_array_3d = apply_distance_threshold_3d(original_array_3d, threshold_distance)\n",
    "print(result_array_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dims = [3, 5, 7, 9, 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select treshold by dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pt_values = list(range(15, 46))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_pd_cm = np.zeros((len(dims), len(pt_values), 2, 2)).astype(int)\n",
    "for inputs, labels in tqdm(valloader):\n",
    "    # compute prediction (probability)\n",
    "    p_hat = sigmoid_fun(model(inputs))[:, 0, :, :].detach().cpu().numpy()\n",
    "\n",
    "    for idim, dim in enumerate(dims):\n",
    "        dist = int(dim / 2)\n",
    "        vals = labels[:, :, :].cpu().numpy()\n",
    "        vals = apply_distance_threshold_3d(vals, dist).flatten()\n",
    "\n",
    "        for i, ptresh in enumerate(pt_values):\n",
    "            # for each treshold compute confusion matrix\n",
    "            y_hat = p_hat >= ptresh / 100\n",
    "            y_hat = apply_distance_threshold_3d(y_hat, dist).flatten()\n",
    "            unique_0, counts_0 = np.unique(y_hat[np.where(vals == 0)], return_counts=True)\n",
    "            unique_1, counts_1 = np.unique(y_hat[np.where(vals == 1)], return_counts=True)\n",
    "            for (u, c) in zip(unique_0, counts_0):\n",
    "                if u:  # pred True\n",
    "                    val_pd_cm[idim, i, 0, 1] += c\n",
    "                else:\n",
    "                    val_pd_cm[idim, i, 0, 0] += c\n",
    "            for (u, c) in zip(unique_1, counts_1):\n",
    "                if u:\n",
    "                    val_pd_cm[idim, i, 1, 1] += c\n",
    "                else:\n",
    "                    val_pd_cm[idim, i, 1, 0] += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_pd = (val_pd_cm[:, :, 0, 0] + val_pd_cm[:, :, 1, 1]) / val_pd_cm.sum(axis=(2,3))\n",
    "prc_pd = val_pd_cm[:, :, 1, 1] / val_pd_cm[:, :, :, 1].sum(axis=2)\n",
    "prc_pd[np.where(prc_pd!=prc_pd)] = 1e-8\n",
    "rcl_pd = val_pd_cm[:, :, 1, 1] / val_pd_cm[:, :, 1, :].sum(axis=2)\n",
    "f1_pd = 2 * prc_pd * rcl_pd / (prc_pd + rcl_pd)\n",
    "f1_pd[np.where(f1_pd!=f1_pd)] = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "itresh_by_dim = np.argmax(f1_pd, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ptresh_by_dim = [pt_values[i] for i in itresh_by_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idim in range(len(dims)):\n",
    "    print(dims[idim])\n",
    "    print(ptresh_by_dim[idim])\n",
    "    print(val_pd_cm[idim, itresh_by_dim[idim], :, :])\n",
    "    print(f1_pd[idim, itresh_by_dim[idim]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_d_cm = np.zeros((len(dims), 2, 2)).astype(int)\n",
    "for inputs, labels in tqdm(testloader):\n",
    "    # compute prediction (probability)\n",
    "    p_hat = sigmoid_fun(model(inputs))[:, 0, :, :].detach().cpu().numpy()\n",
    "\n",
    "    for idim, dim in enumerate(dims):\n",
    "        dist = int(dim / 2)\n",
    "        vals = labels[:, :, :].cpu().numpy()\n",
    "        vals = apply_distance_threshold_3d(vals, dist).flatten()\n",
    "\n",
    "        ptresh = ptresh_by_dim[idim]\n",
    "        y_hat = p_hat >= ptresh / 100\n",
    "        y_hat = apply_distance_threshold_3d(y_hat, dist).flatten()\n",
    "        unique_0, counts_0 = np.unique(y_hat[np.where(vals == 0)], return_counts=True)\n",
    "        unique_1, counts_1 = np.unique(y_hat[np.where(vals == 1)], return_counts=True)\n",
    "        for (u, c) in zip(unique_0, counts_0):\n",
    "            if u:  # pred True\n",
    "                test_d_cm[idim, 0, 1] += c\n",
    "            else:\n",
    "                test_d_cm[idim, 0, 0] += c\n",
    "        for (u, c) in zip(unique_1, counts_1):\n",
    "            if u:\n",
    "                test_d_cm[idim, 1, 1] += c\n",
    "            else:\n",
    "                test_d_cm[idim, 1, 0] += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_d = (test_d_cm[:, 0, 0] + test_d_cm[:, 1, 1]) / test_d_cm.sum(axis=(1,2))\n",
    "prc_d = test_d_cm[:, 1, 1] / test_d_cm[:, :, 1].sum(axis=1)\n",
    "prc_d[np.where(prc_d!=prc_d)] = 1e-8\n",
    "rcl_d = test_d_cm[:, 1, 1] / test_d_cm[:, 1, :].sum(axis=1)\n",
    "f1_d = 2 * prc_d * rcl_d / (prc_d + rcl_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(18, 9), sharex=True)\n",
    "sns.barplot(x=dims, y=acc_d, ax=ax[0, 0], color=sns.color_palette()[0])\n",
    "sns.barplot(x=dims, y=prc_d, ax=ax[0, 1], color=sns.color_palette()[0])\n",
    "sns.barplot(x=dims, y=rcl_d, ax=ax[1, 0], color=sns.color_palette()[0])\n",
    "sns.barplot(x=dims, y=f1_d, ax=ax[1, 1], color=sns.color_palette()[0])\n",
    "ax[0,0].set_title(\"Accuracy\")\n",
    "ax[0,1].set_title(\"Precision\")\n",
    "ax[1,0].set_title(\"Recall\")\n",
    "ax[1,1].set_title(\"F1-Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Predict TIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = 10\n",
    "# test_dataloader_full = torch.utils.data.DataLoader(\n",
    "#     CustomDataset(\n",
    "#         test_data[t:t+54, :, :], \n",
    "#         patches, \n",
    "#         frames_idx,\n",
    "#         ibama_data=ib_array\n",
    "#     ),\n",
    "#     batch_size=1,\n",
    "#     shuffle=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 0\n",
    "# labels_list = []\n",
    "# preds_list = []\n",
    "# for inputs, labels in tqdm(test_dataloader_full):\n",
    "#     # get index info\n",
    "#     idx_patch = index // (test_dataloader_full.dataset.X.shape[0] - test_dataloader_full.dataset.autor_window - test_dataloader_full.dataset.future_window + 1)\n",
    "#     idx_time   = index % (test_dataloader_full.dataset.X.shape[0] - test_dataloader_full.dataset.autor_window - test_dataloader_full.dataset.future_window + 1)\n",
    "#     idx_frames = test_dataloader_full.dataset.frames_idx.loc[test_dataloader_full.dataset.patches[idx_patch]]\n",
    "    \n",
    "#     labels_df = pd.melt(\n",
    "#         pd.DataFrame(labels.cpu().numpy()[0, :, :], index=idx_frames[\"x\"].unique(), columns=idx_frames[\"y\"].unique()),\n",
    "#         ignore_index=False\n",
    "#     ).reset_index().rename(columns={\"index\":\"x\", \"variable\":\"y\",\"value\":\"label\"})\n",
    "    \n",
    "#     preds_df = pd.melt(\n",
    "#         pd.DataFrame(model(inputs).detach().cpu().numpy()[0, 0, :, :], index=idx_frames[\"x\"].unique(), columns=idx_frames[\"y\"].unique()),\n",
    "#         ignore_index=False\n",
    "#     ).reset_index().rename(columns={\"index\":\"x\", \"variable\":\"y\",\"value\":\"pred\"})\n",
    "    \n",
    "#     labels_list.append(labels_df)\n",
    "#     preds_list.append(preds_df)\n",
    "    \n",
    "#     index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df = pd.merge(\n",
    "#     pd.concat(labels_list).groupby([\"x\", \"y\"]).max().reset_index(),\n",
    "#     pd.concat(preds_list).groupby([\"x\", \"y\"]).mean().reset_index(),\n",
    "#     on=[\"x\",\"y\"],\n",
    "#     how=\"outer\",\n",
    "#     validate=\"1:1\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply sigmoid to prediction values\n",
    "# result_df[\"pred\"] = 1 / (1+(-result_df[\"pred\"]).apply(np.exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# grid = gpd.read_file(config.TR_FRAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_with_results = grid.merge(result_df, on=[\"x\",\"y\"], how=\"left\", validate=\"1:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save it as raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# am_bounds = gpd.read_file(config.AMAZON_FRONTIER_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geocube.api.core import make_geocube\n",
    "\n",
    "# # raster data with bands by date\n",
    "# grid_raster_data = make_geocube(\n",
    "#     vector_data=grid_with_results.dropna(),\n",
    "#     resolution=(0.01, 0.01),\n",
    "#     measurements=[\"label\", \"pred\"],\n",
    "#     geom=am_bounds.geometry.item(),\n",
    "#     fill=-1.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_raster_data.rio.to_raster(\"model_pred_example.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nowtimestr = datetime.strftime(datetime.now(), format=\"%Y%m%d%H%M%S\")\n",
    "os.mkdir(nowtimestr)\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), os.path.join(nowtimestr, r\"resunet_class_0.01.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_str = f\"\"\"\n",
    "PREDICTION WINDOW (WEEKS): {FUTURE_WINDOW_PRED}\n",
    "\n",
    "BEST TRESHOLD: {ptreshold}\n",
    "CONFUSION MATRIX[0,0]: {test_cm[0,0]}\n",
    "CONFUSION MATRIX[0,1]: {test_cm[0,1]}\n",
    "CONFUSION MATRIX[1,0]: {test_cm[1,0]}\n",
    "CONFUSION MATRIX[1,1]: {test_cm[1,1]}\n",
    "F1-SCORE (TEST): {f1_test}\n",
    "AUGMENTED DIMENSION SIZES: {list(dims)}\n",
    "\"\"\"\n",
    "\n",
    "for idim, dim in enumerate(dims):\n",
    "    result_str += f\"\"\"\n",
    "    FOR AUG SPATIAL DIM: {dim}\n",
    "    BEST TRESHOLD: {ptresh_by_dim[idim]}\n",
    "    CONFUSION MATRIX[0,0]: {test_d_cm[idim, 0,0]}\n",
    "    CONFUSION MATRIX[0,1]: {test_d_cm[idim, 0,1]}\n",
    "    CONFUSION MATRIX[1,0]: {test_d_cm[idim, 1,0]}\n",
    "    CONFUSION MATRIX[1,1]: {test_d_cm[idim, 1,1]}\n",
    "    F1-SCORE (TEST): {f1_d[idim]}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(nowtimestr, \"results.txt\"), \"w\") as file:\n",
    "    file.write(result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
